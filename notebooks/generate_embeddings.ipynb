{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cda8c3-a428-414e-99ec-de022ef40fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a3171e-52cf-41b2-911e-079d2e16bfaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Import libraries\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "\n",
    "# Settings\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "print(\" Basic libraries loaded!\")\n",
    "print(f\" Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9d15d5-0a30-4bd6-b1f2-23631544f2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers torch sentence-transformers tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730f5064-40e9-4f8e-ac45-7b791f1d6572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Import ML libraries\n",
    "print(\" Loading ML libraries...\")\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "print(f\" PyTorch version: {torch.__version__}\")\n",
    "print(f\" CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"    GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    print(f\"    Using CPU \")\n",
    "    device = 'cpu'\n",
    "\n",
    "print(f\"\\n  Computing device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15c687e-050b-4322-8a38-26519f2dacf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Load preprocessed papers\n",
    "print(\" Loading preprocessed papers...\\n\")\n",
    "\n",
    "# Load the 10K sample we created in Step 2\n",
    "papers = []\n",
    "with open('data/processed/papers_sample_10k.json', 'r') as f:\n",
    "    for line in f:\n",
    "        papers.append(json.loads(line))\n",
    "\n",
    "df = pd.DataFrame(papers)\n",
    "\n",
    "print(f\" Loaded {len(df):,} papers\")\n",
    "print(f\"\\n Dataset info:\")\n",
    "print(f\"   Columns: {len(df.columns)}\")\n",
    "print(f\"   Memory: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "# Show sample\n",
    "print(f\"\\n Sample paper:\")\n",
    "sample = df.iloc[0]\n",
    "print(f\"   ID: {sample['id']}\")\n",
    "print(f\"   Title: {sample['title_clean'][:80]}...\")\n",
    "print(f\"   Abstract length: {len(sample['abstract_clean'])} chars\")\n",
    "print(f\"   Year: {sample['year']}\")\n",
    "print(f\"   Categories: {sample['categories']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cb1aec-17f1-4ac4-8b95-052bd085859a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Load embedding model\n",
    "print(\" Loading SentenceTransformer model...\")\n",
    "\n",
    "# Using fast, efficient model for learning\n",
    "model_name = 'all-MiniLM-L6-v2'\n",
    "model = SentenceTransformer(model_name)\n",
    "\n",
    "print(f\" Model loaded: {model_name}\")\n",
    "print(f\"   Embedding dimension: {model.get_sentence_embedding_dimension()}\")\n",
    "print(f\"   Max sequence length: {model.max_seq_length} tokens\")\n",
    "\n",
    "# Test with one paper\n",
    "print(f\"\\n Testing model...\")\n",
    "test_text = df.iloc[0]['title_clean'] + \" \" + df.iloc[0]['abstract_clean']\n",
    "test_embedding = model.encode(test_text, show_progress_bar=False)\n",
    "\n",
    "print(f\"   Input text length: {len(test_text)} characters\")\n",
    "print(f\"   Output embedding shape: {test_embedding.shape}\")\n",
    "print(f\"   Sample values: {test_embedding[:5]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7402f0e0-351a-4715-aad0-ac92fe778656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Create function to generate embeddings\n",
    "def generate_paper_embedding(paper, model, max_chars=2000):\n",
    "    \"\"\"\n",
    "    Generate embedding for a single paper.\n",
    "    Combines title + abstract.\n",
    "    \n",
    "    Args:\n",
    "        paper: Dictionary with 'title_clean' and 'abstract_clean'\n",
    "        model: SentenceTransformer model\n",
    "        max_chars: Maximum characters to process\n",
    "    \n",
    "    Returns:\n",
    "        numpy array: Embedding vector\n",
    "    \"\"\"\n",
    "    # Combine title and abstract\n",
    "    title = paper.get('title_clean', '')\n",
    "    abstract = paper.get('abstract_clean', '')\n",
    "    text = f\"{title} {abstract}\"\n",
    "    \n",
    "    # Truncate if too long\n",
    "    if len(text) > max_chars:\n",
    "        text = text[:max_chars]\n",
    "    \n",
    "    # Generate embedding\n",
    "    embedding = model.encode(text, show_progress_bar=False)\n",
    "    \n",
    "    return embedding\n",
    "\n",
    "# Test the function\n",
    "print(\" Testing embedding generation function...\\n\")\n",
    "\n",
    "test_paper = df.iloc[0]\n",
    "test_emb = generate_paper_embedding(test_paper, model)\n",
    "\n",
    "print(f\"   Function test successful!\")\n",
    "print(f\"   Input paper: {test_paper['id']}\")\n",
    "print(f\"   Input title: {test_paper['title_clean'][:60]}...\")\n",
    "print(f\"   Output shape: {test_emb.shape}\")\n",
    "print(f\"   Output type: {type(test_emb)}\")\n",
    "print(f\"   Output range: [{test_emb.min():.3f}, {test_emb.max():.3f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38871fa8-6d1b-4c01-8d43-02ba2eafcc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Generate embeddings for all papers\n",
    "print(\" GENERATING EMBEDDINGS FOR ALL PAPERS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total papers: {len(df):,}\")\n",
    "print(f\"Estimated time: ~{len(df)/10:.0f}-{len(df)/8:.0f} minutes\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Prepare batch processing\n",
    "batch_size = 32\n",
    "all_embeddings = []\n",
    "all_paper_ids = []\n",
    "\n",
    "# Prepare all texts for batch processing\n",
    "texts = []\n",
    "for idx, paper in df.iterrows():\n",
    "    text = f\"{paper['title_clean']} {paper['abstract_clean']}\"\n",
    "    # Truncate if needed\n",
    "    if len(text) > 2000:\n",
    "        text = text[:2000]\n",
    "    texts.append(text)\n",
    "    all_paper_ids.append(paper['id'])\n",
    "\n",
    "# Generate embeddings in batches (much faster!)\n",
    "print(\"Processing in batches...\")\n",
    "for i in tqdm(range(0, len(texts), batch_size), desc=\"Batches\"):\n",
    "    batch_texts = texts[i:i + batch_size]\n",
    "    batch_embeddings = model.encode(batch_texts, show_progress_bar=False)\n",
    "    all_embeddings.extend(batch_embeddings)\n",
    "\n",
    "# Convert to numpy array\n",
    "embeddings = np.array(all_embeddings)\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(f\" EMBEDDINGS GENERATED SUCCESSFULLY!\")\n",
    "print(f\"=\" * 60)\n",
    "print(f\"   Shape: {embeddings.shape}\")\n",
    "print(f\"   Data type: {embeddings.dtype}\")\n",
    "print(f\"   Memory size: {embeddings.nbytes / (1024**2):.2f} MB\")\n",
    "print(f\"   Value range: [{embeddings.min():.3f}, {embeddings.max():.3f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379b352a-e2ab-4077-b93f-c200b70b17d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Save embeddings to disk\n",
    "\n",
    "# Create embeddings directory\n",
    "os.makedirs('data/embeddings', exist_ok=True)\n",
    "\n",
    "# Save embeddings as numpy array\n",
    "embeddings_file = 'data/embeddings/paper_embeddings_10k.npy'\n",
    "np.save(embeddings_file, embeddings)\n",
    "print(f\" Saved: {embeddings_file}\")\n",
    "print(f\"   Size: {os.path.getsize(embeddings_file) / (1024**2):.2f} MB\")\n",
    "\n",
    "# Save paper IDs (to map embeddings back to papers)\n",
    "ids_file = 'data/embeddings/paper_ids_10k.json'\n",
    "with open(ids_file, 'w') as f:\n",
    "    json.dump(all_paper_ids, f)\n",
    "print(f\" Saved: {ids_file}\")\n",
    "\n",
    "# Save metadata\n",
    "metadata = {\n",
    "    'model_name': model_name,\n",
    "    'num_papers': len(embeddings),\n",
    "    'embedding_dimension': embeddings.shape[1],\n",
    "    'date_created': pd.Timestamp.now().isoformat(),\n",
    "    'batch_size': batch_size\n",
    "}\n",
    "\n",
    "metadata_file = 'data/embeddings/metadata_10k.json'\n",
    "with open(metadata_file, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "print(f\" Saved: {metadata_file}\")\n",
    "\n",
    "print(f\"\\n All files saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf138ac-8b0e-4261-b988-c2345dff02b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Test similarity search\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def find_similar_papers(query_idx, embeddings, df, top_k=5):\n",
    "    \"\"\"\n",
    "    Find papers most similar to the query paper.\n",
    "    \n",
    "    Args:\n",
    "        query_idx: Index of query paper\n",
    "        embeddings: All paper embeddings\n",
    "        df: DataFrame with paper info\n",
    "        top_k: Number of similar papers to return\n",
    "    \n",
    "    Returns:\n",
    "        List of similar papers with similarity scores\n",
    "    \"\"\"\n",
    "    # Get query embedding\n",
    "    query_embedding = embeddings[query_idx].reshape(1, -1)\n",
    "    \n",
    "    # Calculate cosine similarity with all papers\n",
    "    similarities = cosine_similarity(query_embedding, embeddings)[0]\n",
    "    \n",
    "    # Get top k most similar (excluding the query paper itself)\n",
    "    similar_indices = np.argsort(similarities)[::-1][1:top_k+1]\n",
    "    \n",
    "    # Prepare results\n",
    "    results = []\n",
    "    for idx in similar_indices:\n",
    "        results.append({\n",
    "            'index': int(idx),\n",
    "            'paper_id': df.iloc[idx]['id'],\n",
    "            'title': df.iloc[idx]['title_clean'],\n",
    "            'similarity': float(similarities[idx]),\n",
    "            'year': int(df.iloc[idx]['year']),\n",
    "            'categories': df.iloc[idx]['categories']\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test with a paper about machine learning\n",
    "print(\"=\" * 80)\n",
    "print(\" SIMILARITY SEARCH TEST\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Pick a test paper (you can change this index!)\n",
    "test_idx = 42\n",
    "query_paper = df.iloc[test_idx]\n",
    "\n",
    "print(f\"\\nðŸ“„ QUERY PAPER #{test_idx}:\")\n",
    "print(f\"   ID: {query_paper['id']}\")\n",
    "print(f\"   Title: {query_paper['title_clean']}\")\n",
    "print(f\"   Year: {query_paper['year']}\")\n",
    "print(f\"   Categories: {query_paper['categories']}\")\n",
    "print(f\"   Abstract: {query_paper['abstract_clean'][:250]}...\")\n",
    "\n",
    "# Find similar papers\n",
    "similar_papers = find_similar_papers(test_idx, embeddings, df, top_k=5)\n",
    "\n",
    "print(f\"\\n TOP 5 MOST SIMILAR PAPERS:\\n\")\n",
    "\n",
    "for i, paper in enumerate(similar_papers, 1):\n",
    "    print(f\"{i}. [{paper['similarity']:.3f}] {paper['title']}\")\n",
    "    print(f\"   Year: {paper['year']} | Categories: {paper['categories']}\")\n",
    "    print()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\" Similarity ranges from 0 (unrelated) to 1 (identical)\")\n",
    "print(\"   Scores > 0.7 = Very similar\")\n",
    "print(\"   Scores 0.5-0.7 = Somewhat similar\") \n",
    "print(\"   Scores < 0.5 = Different topics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f5d4f4-1a1b-4ef2-83b9-9422197b35c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Analyze overall similarity patterns\n",
    "print(\" ANALYZING SIMILARITY PATTERNS\\n\")\n",
    "\n",
    "# Calculate similarities for a random sample\n",
    "sample_size = 100\n",
    "sample_indices = np.random.choice(len(embeddings), sample_size, replace=False)\n",
    "\n",
    "all_similarities = []\n",
    "for idx in tqdm(sample_indices, desc=\"Calculating similarities\"):\n",
    "    query_emb = embeddings[idx].reshape(1, -1)\n",
    "    sims = cosine_similarity(query_emb, embeddings)[0]\n",
    "    # Exclude self-similarity (which is always 1.0)\n",
    "    sims = sims[sims < 0.999]\n",
    "    all_similarities.extend(sims)\n",
    "\n",
    "all_similarities = np.array(all_similarities)\n",
    "\n",
    "print(f\"\\nðŸ“ˆ Similarity Statistics (from {sample_size} papers):\")\n",
    "print(f\"   Mean: {all_similarities.mean():.3f}\")\n",
    "print(f\"   Median: {np.median(all_similarities):.3f}\")\n",
    "print(f\"   Std: {all_similarities.std():.3f}\")\n",
    "print(f\"   Min: {all_similarities.min():.3f}\")\n",
    "print(f\"   Max: {all_similarities.max():.3f}\")\n",
    "\n",
    "# Plot distribution\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(all_similarities, bins=50, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "plt.axvline(all_similarities.mean(), color='red', linestyle='--', \n",
    "            linewidth=2, label=f'Mean: {all_similarities.mean():.3f}')\n",
    "plt.xlabel('Cosine Similarity', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.title('Distribution of Paper Similarities', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.boxplot(all_similarities, vert=True)\n",
    "plt.ylabel('Cosine Similarity', fontsize=12)\n",
    "plt.title('Similarity Boxplot', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Interpretation\n",
    "print(f\"\\n Interpretation:\")\n",
    "if all_similarities.mean() > 0.3:\n",
    "    print(f\"     High average similarity - papers might be too similar\")\n",
    "elif all_similarities.mean() < 0.1:\n",
    "    print(f\"     Low average similarity - embeddings might need tuning\")\n",
    "else:\n",
    "    print(f\"    Good distribution - embeddings capture meaningful differences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757f2d20-0297-4209-b5f3-94d5f0855f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Visualize embeddings in 2D space\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "print(\" Creating 2D visualization of embeddings...\")\n",
    "\n",
    "# Use subset for visualization (1000 papers)\n",
    "viz_sample_size = 1000\n",
    "viz_indices = np.random.choice(len(embeddings), viz_sample_size, replace=False)\n",
    "embeddings_sample = embeddings[viz_indices]\n",
    "df_viz = df.iloc[viz_indices].reset_index(drop=True)\n",
    "\n",
    "# Reduce dimensions with t-SNE\n",
    "print(\"   Running t-SNE dimensionality reduction...\")\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30, max_iter=1000)\n",
    "embeddings_2d = tsne.fit_transform(embeddings_sample)\n",
    "\n",
    "print(\"   Creating visualization...\")\n",
    "\n",
    "# Extract main category for coloring\n",
    "df_viz['main_category'] = df_viz['categories'].apply(\n",
    "    lambda x: x.split()[0] if isinstance(x, str) else 'unknown'\n",
    ")\n",
    "\n",
    "# Get top 6 categories\n",
    "top_cats = df_viz['main_category'].value_counts().head(6).index.tolist()\n",
    "df_viz['plot_category'] = df_viz['main_category'].apply(\n",
    "    lambda x: x if x in top_cats else 'Other'\n",
    ")\n",
    "\n",
    "# Create beautiful scatter plot\n",
    "plt.figure(figsize=(16, 10))\n",
    "\n",
    "categories = df_viz['plot_category'].unique()\n",
    "colors = plt.cm.Set3(np.linspace(0, 1, len(categories)))\n",
    "\n",
    "for category, color in zip(categories, colors):\n",
    "    mask = df_viz['plot_category'] == category\n",
    "    plt.scatter(\n",
    "        embeddings_2d[mask, 0],\n",
    "        embeddings_2d[mask, 1],\n",
    "        c=[color],\n",
    "        label=category,\n",
    "        alpha=0.6,\n",
    "        s=50,\n",
    "        edgecolors='black',\n",
    "        linewidth=0.5\n",
    "    )\n",
    "\n",
    "plt.title('Research Paper Embeddings Visualization (t-SNE Projection)', \n",
    "          fontsize=18, fontweight='bold', pad=20)\n",
    "plt.xlabel('t-SNE Dimension 1', fontsize=14)\n",
    "plt.ylabel('t-SNE Dimension 2', fontsize=14)\n",
    "plt.legend(title='Category', bbox_to_anchor=(1.05, 1), \n",
    "           loc='upper left', fontsize=12, title_fontsize=13)\n",
    "plt.grid(True, alpha=0.2)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n Visualization complete!\")\n",
    "print(\"   - Each dot = one paper\")\n",
    "print(\"   - Close dots = similar content\")\n",
    "print(\"   - Colors = paper categories\")\n",
    "print(\"   - how papers cluster by topic!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca9ef11-5488-48bd-b9ca-e30d39c75357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Analyze how well categories cluster\n",
    "print(\" CATEGORY CLUSTERING ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "# Sample papers from each major category\n",
    "major_categories = df['categories'].apply(lambda x: x.split()[0]).value_counts().head(5).index\n",
    "\n",
    "category_quality = {}\n",
    "\n",
    "for category in tqdm(major_categories, desc=\"Analyzing categories\"):\n",
    "    # Get papers in this category\n",
    "    cat_papers = df[df['categories'].str.startswith(category)]\n",
    "    \n",
    "    if len(cat_papers) < 10:\n",
    "        continue\n",
    "    \n",
    "    # Sample 20 papers from this category\n",
    "    sample_size = min(20, len(cat_papers))\n",
    "    sampled = cat_papers.sample(sample_size, random_state=42)\n",
    "    \n",
    "    # For each paper, find its 10 nearest neighbors\n",
    "    same_category_counts = []\n",
    "    \n",
    "    for idx in sampled.index:\n",
    "        paper_idx = df.index.get_loc(idx)\n",
    "        similar = find_similar_papers(paper_idx, embeddings, df, top_k=10)\n",
    "        \n",
    "        # Count how many are in the same category\n",
    "        same_cat = sum(1 for s in similar if s['categories'].startswith(category))\n",
    "        same_category_counts.append(same_cat / 10)\n",
    "    \n",
    "    # Average precision for this category\n",
    "    avg_precision = np.mean(same_category_counts)\n",
    "    category_quality[category] = avg_precision\n",
    "\n",
    "# Display results\n",
    "print(f\"\\nðŸ“Š Category Clustering Quality:\")\n",
    "print(f\"   (Higher = better category separation)\\n\")\n",
    "\n",
    "for cat, score in sorted(category_quality.items(), key=lambda x: x[1], reverse=True):\n",
    "    bar = 'â–ˆ' * int(score * 20)\n",
    "    print(f\"   {cat:20} : {score:.2%} {bar}\")\n",
    "\n",
    "overall_quality = np.mean(list(category_quality.values()))\n",
    "print(f\"\\n   Overall Average: {overall_quality:.2%}\")\n",
    "\n",
    "if overall_quality > 0.6:\n",
    "    print(f\"\\n    Excellent! Embeddings group similar papers well!\")\n",
    "elif overall_quality > 0.4:\n",
    "    print(f\"\\n    Good! Embeddings are working correctly!\")\n",
    "else:\n",
    "    print(f\"\\n     Moderate - but okay for learning purposes!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3dbaa61-37fe-41e2-90a1-dd0b1d4fc143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: Complete summary\n",
    "\n",
    "print(f\"\\n Final Statistics:\")\n",
    "print(f\"   Papers processed: {len(embeddings):,}\")\n",
    "print(f\"   Embedding dimension: {embeddings.shape[1]}\")\n",
    "print(f\"   Model used: {model_name}\")\n",
    "print(f\"   Total embeddings size: {embeddings.nbytes / (1024**2):.2f} MB\")\n",
    "print(f\"   Average similarity: {all_similarities.mean():.3f}\")\n",
    "print(f\"   Category clustering: {overall_quality:.2%}\")\n",
    "\n",
    "print(f\"\\n Files Created:\")\n",
    "print(f\"    data/embeddings/paper_embeddings_10k.npy ({embeddings.nbytes / (1024**2):.2f} MB)\")\n",
    "print(f\"    data/embeddings/paper_ids_10k.json\")\n",
    "print(f\"    data/embeddings/metadata_10k.json\")\n",
    "\n",
    "# Quick sanity check\n",
    "test_idx = np.random.randint(0, len(df))\n",
    "test_paper = df.iloc[test_idx]\n",
    "similar = find_similar_papers(test_idx, embeddings, df, top_k=3)\n",
    "\n",
    "print(f\"\\n   Query: {test_paper['title_clean'][:60]}...\")\n",
    "print(f\"   Top match: {similar[0]['title'][:60]}...\")\n",
    "print(f\"   Similarity: {similar[0]['similarity']:.3f}\")\n",
    "\n",
    "if similar[0]['similarity'] > 0.5:\n",
    "    print(f\"\\n    Embeddings are working correctly!\")\n",
    "else:\n",
    "    print(f\"\\n     Lower similarity - but still usable!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a009203e-b423-4c06-a9f7-da3402d504a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
