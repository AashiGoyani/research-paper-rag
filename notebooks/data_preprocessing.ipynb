{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df522c2-6674-4b59-bd93-1a766032fd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Import libraries\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import re\n",
    "import os\n",
    "\n",
    "# Settings\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "print(\"Libraries loaded!\")\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910246f4-0bdd-4580-8cb4-97fe82d6e478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Count total papers in dataset\n",
    "print(\" Counting total papers in dataset...\")\n",
    "print(\"(This takes 1-2 minutes for the full file)\\n\")\n",
    "\n",
    "total_papers = 0\n",
    "with open('/Users/aashigoyani/research-paper-recommender/rpr/data/raw/arxiv-metadata-oai-snapshot.json', 'r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        total_papers += 1\n",
    "        \n",
    "        # Progress update every 100k papers\n",
    "        if (i + 1) % 100000 == 0:\n",
    "            print(f\"Counted {i + 1:,} papers...\")\n",
    "\n",
    "print(f\"\\nTotal papers in dataset: {total_papers:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7680c8-f8e9-41ee-a482-a0bf30822a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: What categories do we have?\n",
    "print(\" Analyzing category distribution...\")\n",
    "print(\"(Loading first 50,000 papers to sample)\\n\")\n",
    "\n",
    "category_counter = Counter()\n",
    "\n",
    "with open('/Users/aashigoyani/research-paper-recommender/rpr/data/raw/arxiv-metadata-oai-snapshot.json', 'r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i >= 50000:  # Sample first 50K\n",
    "            break\n",
    "            \n",
    "        paper = json.loads(line)\n",
    "        categories = paper['categories'].split()\n",
    "        category_counter.update(categories)\n",
    "        \n",
    "        if (i + 1) % 10000 == 0:\n",
    "            print(f\"Processed {i + 1:,} papers...\")\n",
    "\n",
    "# Show top categories\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TOP 20 CATEGORIES (from 50K sample)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for cat, count in category_counter.most_common(20):\n",
    "    percentage = (count / 50000) * 100\n",
    "    print(f\"{cat:20} : {count:6,} papers ({percentage:5.2f}%)\")\n",
    "\n",
    "# Identify CS categories\n",
    "cs_categories = [cat for cat in category_counter.keys() if cat.startswith('cs.')]\n",
    "print(f\"\\nFound {len(cs_categories)} Computer Science categories:\")\n",
    "print(cs_categories[:10])  # Show first 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bafa2f7-6ea0-431f-9d49-b2112afba119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Extract Computer Science papers only\n",
    "\n",
    "cs_papers = []\n",
    "total_scanned = 0\n",
    "\n",
    "with open('/Users/aashigoyani/research-paper-recommender/rpr/data/raw/arxiv-metadata-oai-snapshot.json', 'r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        paper = json.loads(line)\n",
    "        total_scanned += 1\n",
    "        \n",
    "        # Check if any category starts with 'cs.'\n",
    "        categories = paper['categories'].split()\n",
    "        if any(cat.startswith('cs.') for cat in categories):\n",
    "            cs_papers.append(paper)\n",
    "        \n",
    "        # Progress update\n",
    "        if (i + 1) % 100000 == 0:\n",
    "            print(f\"Scanned {i + 1:,} papers | Found {len(cs_papers):,} CS papers\")\n",
    "        \n",
    "        # Optional: Stop after finding 100K CS papers (remove this if you want all)\n",
    "        if len(cs_papers) >= 100000:\n",
    "            print(f\"\\nâ¸  Stopping at 100,000 CS papers for manageable size\")\n",
    "            break\n",
    "\n",
    "print(f\"\\n Total scanned: {total_scanned:,}\")\n",
    "print(f\"CS papers found: {len(cs_papers):,}\")\n",
    "print(f\"Percentage: {(len(cs_papers)/total_scanned)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54926ae-16af-4f54-abe3-b285f27a647a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Convert to DataFrame\n",
    "print(\"Converting to DataFrame...\")\n",
    "\n",
    "df_cs = pd.DataFrame(cs_papers)\n",
    "\n",
    "print(f\" DataFrame created!\")\n",
    "print(f\"   Shape: {df_cs.shape}\")\n",
    "print(f\"   Columns: {df_cs.columns.tolist()}\\n\")\n",
    "\n",
    "# Show info\n",
    "print(\"=\" * 60)\n",
    "print(\"DATASET INFO\")\n",
    "print(\"=\" * 60)\n",
    "df_cs.info()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SAMPLE RECORDS\")\n",
    "print(\"=\" * 60)\n",
    "df_cs.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96da07b5-5673-420b-940c-5a63e1c8fba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Clean the data\n",
    "print(\" Cleaning data...\\n\")\n",
    "\n",
    "# Store original size\n",
    "original_size = len(df_cs)\n",
    "\n",
    "# 1. Remove duplicates by paper ID\n",
    "df_cs = df_cs.drop_duplicates(subset=['id'], keep='first')\n",
    "print(f\"1. Removed {original_size - len(df_cs)} duplicate papers\")\n",
    "\n",
    "# 2. Remove papers with very short abstracts (likely incomplete)\n",
    "df_cs['abstract_length'] = df_cs['abstract'].str.len()\n",
    "df_cs = df_cs[df_cs['abstract_length'] >= 100]\n",
    "print(f\"2. Removed papers with abstracts < 100 characters\")\n",
    "\n",
    "# 3. Remove papers with missing titles\n",
    "df_cs = df_cs[df_cs['title'].notna()]\n",
    "print(f\"3. Removed papers with missing titles\")\n",
    "\n",
    "# 4. Extract year from update_date\n",
    "df_cs['year'] = pd.to_datetime(df_cs['update_date']).dt.year\n",
    "print(f\"4. Extracted year from dates\")\n",
    "\n",
    "# 5. Clean text fields (remove extra whitespace, LaTeX commands)\n",
    "def clean_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    # Remove LaTeX commands\n",
    "    text = re.sub(r'\\$.*?\\$', '', text)  # Remove inline math\n",
    "    text = re.sub(r'\\\\[a-zA-Z]+\\{.*?\\}', '', text)  # Remove LaTeX commands\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "df_cs['title_clean'] = df_cs['title'].apply(clean_text)\n",
    "df_cs['abstract_clean'] = df_cs['abstract'].apply(clean_text)\n",
    "print(f\"5. Cleaned text fields\")\n",
    "\n",
    "print(f\"\\n Final dataset size: {len(df_cs):,} papers\")\n",
    "print(f\"   Removed {original_size - len(df_cs):,} papers total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60a3e67-1ab4-445c-aa7f-3b1cc68fc22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Analyze the cleaned dataset\n",
    "print(\"=\" * 60)\n",
    "print(\"CLEANED DATASET STATISTICS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Year distribution\n",
    "print(\"\\n Papers by Year (Last 10 years):\")\n",
    "year_dist = df_cs['year'].value_counts().sort_index().tail(10)\n",
    "for year, count in year_dist.items():\n",
    "    print(f\"   {year}: {count:,} papers\")\n",
    "\n",
    "# Category distribution\n",
    "print(\"\\n  Top CS Sub-categories:\")\n",
    "cs_subcats = Counter()\n",
    "for cats in df_cs['categories']:\n",
    "    cs_cats = [c for c in cats.split() if c.startswith('cs.')]\n",
    "    cs_subcats.update(cs_cats)\n",
    "\n",
    "for cat, count in cs_subcats.most_common(10):\n",
    "    percentage = (count / len(df_cs)) * 100\n",
    "    print(f\"   {cat:15} : {count:6,} papers ({percentage:5.2f}%)\")\n",
    "\n",
    "# Abstract length\n",
    "print(f\"\\nðŸ“ Abstract Length Statistics:\")\n",
    "print(f\"   Mean: {df_cs['abstract_length'].mean():.0f} characters\")\n",
    "print(f\"   Median: {df_cs['abstract_length'].median():.0f} characters\")\n",
    "print(f\"   Min: {df_cs['abstract_length'].min():.0f} characters\")\n",
    "print(f\"   Max: {df_cs['abstract_length'].max():.0f} characters\")\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Year distribution\n",
    "year_dist.plot(kind='bar', ax=axes[0], color='steelblue')\n",
    "axes[0].set_title('Papers by Year', fontweight='bold', fontsize=12)\n",
    "axes[0].set_xlabel('Year')\n",
    "axes[0].set_ylabel('Number of Papers')\n",
    "\n",
    "# Abstract length distribution\n",
    "axes[1].hist(df_cs['abstract_length'], bins=50, color='coral', edgecolor='black')\n",
    "axes[1].set_title('Abstract Length Distribution', fontweight='bold', fontsize=12)\n",
    "axes[1].set_xlabel('Length (characters)')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].axvline(df_cs['abstract_length'].mean(), color='red', \n",
    "                linestyle='--', label='Mean')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b0a618-41d6-4d61-aaab-8ef6a60ef223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Create manageable sample for development\n",
    "print(\" Creating sample datasets...\\n\")\n",
    "\n",
    "# Create different sized samples\n",
    "sample_sizes = {\n",
    "    'small': 10000,\n",
    "    'medium': 50000,\n",
    "    'large': 100000\n",
    "}\n",
    "\n",
    "samples = {}\n",
    "for name, size in sample_sizes.items():\n",
    "    if len(df_cs) >= size:\n",
    "        # Random sample\n",
    "        sample = df_cs.sample(n=size, random_state=42)\n",
    "        samples[name] = sample\n",
    "        print(f\" Created '{name}' sample: {size:,} papers\")\n",
    "    else:\n",
    "        print(f\"  Dataset smaller than {size:,}, using all {len(df_cs):,} papers\")\n",
    "        samples[name] = df_cs\n",
    "\n",
    "# We'll work with the small sample (10K) for development\n",
    "df_sample = samples['small']\n",
    "\n",
    "print(f\"\\n Working with: {len(df_sample):,} papers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d77ed9-383c-4271-a3e1-289ca70c9214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Save processed datasets\n",
    "import os\n",
    "\n",
    "# Create processed data directory if it doesn't exist\n",
    "os.makedirs('data/processed', exist_ok=True)\n",
    "\n",
    "print(\" Saving processed data...\\n\")\n",
    "\n",
    "# Save the sample (for quick development)\n",
    "df_sample.to_json('data/processed/papers_sample_10k.json', \n",
    "                  orient='records', \n",
    "                  lines=True)\n",
    "print(f\"Saved: data/processed/papers_sample_10k.json ({len(df_sample):,} papers)\")\n",
    "\n",
    "# Save medium sample if exists\n",
    "if 'medium' in samples and len(samples['medium']) > len(df_sample):\n",
    "    samples['medium'].to_json('data/processed/papers_sample_50k.json',\n",
    "                              orient='records',\n",
    "                              lines=True)\n",
    "    print(f\"Saved: data/processed/papers_sample_50k.json ({len(samples['medium']):,} papers)\")\n",
    "\n",
    "# Save full CS dataset\n",
    "df_cs.to_json('data/processed/papers_cs_full.json',\n",
    "              orient='records',\n",
    "              lines=True)\n",
    "print(f\"Saved: data/processed/papers_cs_full.json ({len(df_cs):,} papers)\")\n",
    "\n",
    "# Save a CSV version of the sample for easy viewing\n",
    "df_sample[['id', 'title_clean', 'authors', 'year', 'categories']].to_csv(\n",
    "    'data/processed/papers_sample_10k_preview.csv',\n",
    "    index=False\n",
    ")\n",
    "print(f\"Saved: data/processed/papers_sample_10k_preview.csv (for preview)\")\n",
    "\n",
    "print(f\"\\n All data saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2fd069-97c7-4727-9e5d-2a9158d38284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Final summary\n",
    "print(\"=\" * 60)\n",
    "print(\" DATA PREPROCESSING COMPLETE!\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\n Processed Files Created:\")\n",
    "print(f\"   1. papers_sample_10k.json         - {len(df_sample):,} papers (for development)\")\n",
    "print(f\"   2. papers_cs_full.json            - {len(df_cs):,} papers (full CS dataset)\")\n",
    "print(f\"   3. papers_sample_10k_preview.csv  - Preview in spreadsheet format\")\n",
    "\n",
    "print(f\"\\n Dataset Statistics:\")\n",
    "print(f\"   Total papers scanned: {total_scanned:,}\")\n",
    "print(f\"   CS papers extracted: {len(df_cs):,}\")\n",
    "print(f\"   Year range: {df_cs['year'].min()} - {df_cs['year'].max()}\")\n",
    "print(f\"   Unique categories: {len(cs_subcats)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bf5955-15ed-4761-9464-3965a36eff4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Research Paper)",
   "language": "python",
   "name": "research-paper-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
